{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2604ab9f-0a2d-47eb-8f21-06850b0e4a3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/z5173707/root/projects/FHS/venv/python3.12.6/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "from os.path import splitext, join\n",
    "from collections import defaultdict\n",
    "\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "import torch\n",
    "\n",
    "\n",
    "import gradio as gr\n",
    "from gradio.components import Audio, Dropdown, Textbox, Image\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import txtgrid_master.TextGrid_Master as tm\n",
    "from core.utils import generate_file_basename, load_speech_file, zip_files\n",
    "from config import output_dir, MAX_DUR, ASR_MODEL, SPEECH_LABEL, LM_PATH, device\n",
    "\n",
    "from tasks.sd import pyannote_sd\n",
    "from tasks.vad import pyannote_vad\n",
    "from tasks.asr import wav2vec_asr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2d740903-bae1-406e-807f-a413d1623656",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_asr(params=None):\n",
    "    #TODO add to the asr class to read parameters and load the correct model? or a separate function\n",
    "\n",
    "    try:\n",
    "        asr_engine = wav2vec_asr.speech_recognition(ASR_MODEL, device= device, lm_model_path=LM_PATH) #This from parameters and has default one #If language not determine use language id\n",
    "    except:\n",
    "        print(f'Error loading asr model {ASR_MODEL}')\n",
    "        raise \"Error in loading asr model\"\n",
    "    \n",
    "    return asr_engine\n",
    "\n",
    "\n",
    "\n",
    "def load_sd(params=None):\n",
    "    \n",
    "    try:\n",
    "        diarizer = pyannote_sd.speaker_diar(device=device)\n",
    "    except Exception as e:\n",
    "        print(f'Error loading speaker diarization model')\n",
    "        raise f\"Error in loading speaker diarization model {e}\"\n",
    "    \n",
    "    return diarizer\n",
    "\n",
    "\n",
    "\n",
    "vad_params = {\n",
    "             'min_duration_off': 0.09791355693027545,\n",
    "             'min_duration_on': 0.05537587440407595\n",
    "             }\n",
    "\n",
    "def load_vad(params=None):\n",
    "    model_path = 'Models/VAD/pytorch_model.bin'\n",
    "    try:\n",
    "        vad_pipeline = pyannote_vad.speech_detection(model_path, params)\n",
    "    except:\n",
    "        print(f'Error loading voice activity detection model {model_path}')\n",
    "        raise \"Error in loading voice activity detection model\"   \n",
    "    return vad_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e209d23b-a26b-4d69-9404-ff4096db0cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_file(speech_file, tasks=['SD', 'ASR'], parameters=None, progress=gr.Progress()):\n",
    "    basename = generate_file_basename()\n",
    "    \n",
    "    speech, sr, duration = load_speech_file(speech_file)\n",
    "    \n",
    "    #This save a version of the speech file with 16k, mono, 16bit\n",
    "    speech_file = join(output_dir,f'{basename}.wav')\n",
    "    sf.write(speech_file, speech, sr)\n",
    "    \n",
    "    tasks = set(tasks)\n",
    "    \n",
    "    task_pipeline = []\n",
    "    if len(tasks) == 1:\n",
    "        if 'ASR' in tasks and duration > MAX_DUR: #Add VAD task to split the speech file by sil\n",
    "            task_pipeline = ['VAD', 'ASR']\n",
    "        else:\n",
    "            task_pipeline = list(tasks)\n",
    "    elif set(tasks) == set(['SD', 'ASR']):\n",
    "        task_pipeline = ['SD', 'ASR']\n",
    "    elif set(tasks) == set(['VAD', 'ASR']):\n",
    "        task_pipeline = ['VAD', 'ASR']\n",
    "    elif set(tasks) == set(['VAD', 'ASR', 'SD']): #If both SD, VAD and ASR then ASR will be applied on SD output\n",
    "        task_pipeline = ['VAD', 'SD', 'ASR']\n",
    "    else:\n",
    "        task_pipeline = tasks #Only 'SD' and 'VAD' each one will be applied separetly\n",
    "\n",
    "    #print(speech_file, tasks, duration)\n",
    "    if 'ASR' in task_pipeline:\n",
    "        asr_engine = load_asr()\n",
    "    \n",
    "    if 'SD' in task_pipeline:\n",
    "        diarizer = load_sd()\n",
    "        \n",
    "    if 'VAD' in task_pipeline:\n",
    "        vad_engine = load_vad(params=vad_params)\n",
    "    \n",
    "    out_textgrid = []\n",
    "    \n",
    "    i = 0\n",
    "    for task in task_pipeline:\n",
    "        progress(i/(len(task_pipeline)+1), desc=f\"Applying {task}\")\n",
    "        i = i+1\n",
    "        if task == 'ASR':\n",
    "            texgrid_file = join(output_dir,f'{basename}_ASR.TextGrid')\n",
    "            if not out_textgrid:\n",
    "                dTiers_asr = asr_engine.process_speech(speech)\n",
    "            else:\n",
    "                input_textgrid = out_textgrid[-1]\n",
    "                dTiers_asr = asr_engine.process_intervals(speech, input_textgrid, sr = sr, offset_sec=0, speech_label = SPEECH_LABEL)\n",
    "            \n",
    "            tm.WriteTxtGrdFromDict(texgrid_file,dTiers_asr,0,duration)\n",
    "            out_textgrid.append(texgrid_file)\n",
    "        \n",
    "        elif task == 'VAD':\n",
    "            rttm_file = join(output_dir,f'{basename}_VAD.rttm')\n",
    "            texgrid_file = join(output_dir,f'{basename}_VAD.TextGrid')\n",
    "            vad_engine.DoVAD(speech,sr)\n",
    "            vad_engine.write_rttm(rttm_file)\n",
    "            vad_engine.write_textgrid(texgrid_file, speech_label=SPEECH_LABEL)\n",
    "            out_textgrid.append(texgrid_file)\n",
    "        \n",
    "        elif task == 'SD':\n",
    "            rttm_file = join(output_dir,f'{basename}_SD.rttm')\n",
    "            texgrid_file = join(output_dir,f'{basename}_SD.TextGrid')\n",
    "            diarizer.diarize(speech=speech, sr=sr)\n",
    "            diarizer.write_rttm(rttm_file)\n",
    "            diarizer.write_textgrid(texgrid_file, speech_label=SPEECH_LABEL)\n",
    "            out_textgrid.append(texgrid_file)\n",
    "    \n",
    "    \n",
    "    output_zip_file = join(output_dir,f'{basename}_output.zip')\n",
    "    \n",
    "    progress(len(task_pipeline)/(len(task_pipeline)+1), desc=f\"Create output archive\")\n",
    "    p = zip_files(out_textgrid, output_zip_file)\n",
    "    \n",
    "    progress(1, desc=p)\n",
    "    \n",
    "    return [p, gr.DownloadButton(label=f\"Download output file\", value=output_zip_file, visible=True), \n",
    "           gr.DownloadButton(label=f\"Download speech file\", value=speech_file, visible=True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e79f9b4b-b383-4360-b53a-59ad148fc80a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7864\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7864/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/z5173707/root/projects/FHS/venv/python3.12.6/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at /Users/z5173707/root/projects/FHS/alKhalilVox/Models/ASR/wav2vec2-large-xlsr-53-english/ were not used when initializing Wav2Vec2ForCTC: ['wav2vec2.encoder.pos_conv_embed.conv.weight_g', 'wav2vec2.encoder.pos_conv_embed.conv.weight_v']\n",
      "- This IS expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at /Users/z5173707/root/projects/FHS/alKhalilVox/Models/ASR/wav2vec2-large-xlsr-53-english/ and are newly initialized: ['wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Loading the LM will be faster if you build a binary file.\n",
      "Reading /Users/z5173707/root/projects/FHS/alKhalilVox/Models/ASR/LM/ngram/4gram_big.arpa\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "****************************************************************************************************\n",
      "Found entries of length > 1 in alphabet. This is unusual unless style is BPE, but the alphabet was not recognized as BPE type. Is this correct?\n",
      "Unigrams and labels don't seem to agree.\n"
     ]
    }
   ],
   "source": [
    "#TODO: Progress bar\n",
    "#TODO: VAD ########DONE\n",
    "#TODO: Number of speakers\n",
    "#TODO: Word alignment\n",
    "#TODO: Download button  ######DONE\n",
    "#TODO: Wrape in a docker #####DONE\n",
    "#TODO: Test pyannote offline  ########DONE\n",
    "#TODO: Logging of errors and info\n",
    "#TODO: ASR with LM  ########DONE\n",
    "#TODO: Process batch\n",
    "#TODO: Kaldi ASR\n",
    "#TODO: MMS ASR\n",
    "#TODO: Parameters for ASR (hotwords, LM/noLM)\n",
    "\n",
    "\n",
    "\n",
    "with gr.Blocks(theme=gr.themes.Soft()) as gui:\n",
    "\n",
    "    record_audio = gr.Audio(sources=[\"microphone\",\"upload\"], type=\"filepath\")\n",
    "\n",
    "    tasks = gr.CheckboxGroup(choices=[(\"Speech to Text\",\"ASR\"), (\"Speaker Separation\",\"SD\"),(\"Speech Detection\",\"VAD\")], label=\"Tasks\", info=\"Apply the following tasks:\")\n",
    "\n",
    "    process = gr.Button(\"Process Audio\")\n",
    "\n",
    "    output_text = gr.Textbox(label='Progress', interactive=False)\n",
    "    with gr.Row():\n",
    "        with gr.Column():\n",
    "            d1 = gr.DownloadButton(\"Download output\", visible=False)\n",
    "        with gr.Column():\n",
    "            d2 = gr.DownloadButton(\"Download speech file\", visible=False)\n",
    "\n",
    "    process.click(process_file, inputs=[record_audio, tasks], outputs=[output_text, d1, d2])\n",
    "    \n",
    "    \n",
    "     \n",
    "gui.launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa340a2f-2180-41ce-98f9-f71208923baf",
   "metadata": {},
   "source": [
    "# Draft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "05f46b86-47e2-4d0d-8b17-5661f15ad035",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "94ca648c-2cd1-4ebc-8e6e-b18c707b0f84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "e64e8c37-7453-49a7-a469-32cb22cabb4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Wav2Vec2ForCTC, Wav2Vec2Processor, AutoProcessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "95b26577-8375-4997-aac1-1c2b0917a139",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"facebook/mms-1b-all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "db64e313-0022-495b-bcb8-760342118ffb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at facebook/mms-1b-all were not used when initializing Wav2Vec2ForCTC: ['wav2vec2.encoder.pos_conv_embed.conv.weight_g', 'wav2vec2.encoder.pos_conv_embed.conv.weight_v']\n",
      "- This IS expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/mms-1b-all and are newly initialized: ['wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = Wav2Vec2ForCTC.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "29b040cd-61b9-4915-bc0e-5a28a183f15a",
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = AutoProcessor.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "108c82be-08be-4ea2-9b30-02cf06c9d248",
   "metadata": {},
   "outputs": [],
   "source": [
    "processor.tokenizer.set_target_lang(\"eng\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "e93936c7-5f55-40c1-966e-35c2acbbedc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_adapter(\"eng\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "f9e5b89f-5d1b-4263-8424-9380fdac30e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "speech, sr, duration = load_speech_file('../tmp/file_6820_20240928_232234_530279.wav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "a057d9d1-50d2-4f2a-b5e1-d1b2261f8db3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19.26"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "47633af0-b2a8-4f37-9a46-767b0627d7e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = processor(speech, sampling_rate=16_000, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "8e411cf6-8cb9-4ebe-a7b6-f1ebe757c6f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    outputs = model(**inputs).logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "0d3d2a9d-290a-4994-a5a6-622c2c91a74e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = torch.argmax(outputs, dim=-1)[0]\n",
    "transcription = processor.decode(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "e8d1aa54-b2ba-45bb-addd-c66b8a2ee95c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"okay now i will pretend i'm two this is the first speaker tis is a second speaer  first one will say hallo second one will say hallo back\""
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transcription"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "85fbdfd6-c8b6-4640-8524-ade4e8799a35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyctcdecode==0.5.0 in /Users/z5173707/root/projects/FHS/venv/python3.12.6/lib/python3.12/site-packages (0.5.0)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.15.0 in /Users/z5173707/root/projects/FHS/venv/python3.12.6/lib/python3.12/site-packages (from pyctcdecode==0.5.0) (1.26.4)\n",
      "Requirement already satisfied: pygtrie<3.0,>=2.1 in /Users/z5173707/root/projects/FHS/venv/python3.12.6/lib/python3.12/site-packages (from pyctcdecode==0.5.0) (2.5.0)\n",
      "Requirement already satisfied: hypothesis<7,>=6.14 in /Users/z5173707/root/projects/FHS/venv/python3.12.6/lib/python3.12/site-packages (from pyctcdecode==0.5.0) (6.112.2)\n",
      "Requirement already satisfied: attrs>=22.2.0 in /Users/z5173707/root/projects/FHS/venv/python3.12.6/lib/python3.12/site-packages (from hypothesis<7,>=6.14->pyctcdecode==0.5.0) (24.2.0)\n",
      "Requirement already satisfied: sortedcontainers<3.0.0,>=2.1.0 in /Users/z5173707/root/projects/FHS/venv/python3.12.6/lib/python3.12/site-packages (from hypothesis<7,>=6.14->pyctcdecode==0.5.0) (2.4.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install pyctcdecode==0.5.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "30eaac5b-01a9-4182-a450-210e26a73271",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyctcdecode import build_ctcdecoder\n",
    "import pyctcdecode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "4e4e9c64-6f1e-44fe-8a2a-2719121eec12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'pyctcdecode' from '/Users/z5173707/root/projects/FHS/venv/python3.12.6/lib/python3.12/site-packages/pyctcdecode/__init__.py'>"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pyctcdecode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8f3ad489-4f17-402c-98ca-76b0c0093043",
   "metadata": {},
   "outputs": [],
   "source": [
    "lm_path = 'Models/ASR/LM/ngram/4gram_small.arpa.gz'\n",
    "lm_path_unzip = 'Models/ASR/LM/ngram/4gram_small.arpa'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fb48b5e4-0686-42db-aa60-d11e197c74c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import os, shutil\n",
    "with gzip.open(lm_path, 'rb') as f_zipped:\n",
    "    with open(lm_path_unzip, 'wb') as f_unzipped:\n",
    "        shutil.copyfileobj(f_zipped, f_unzipped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c3d703ce-28d6-4e73-b15a-c7d7bebbfa77",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_dict = processor.tokenizer.get_vocab()\n",
    "sorted_vocab_dict = {k.lower(): v for k, v in sorted(vocab_dict.items(), key=lambda item: item[1])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ad625132-d403-4a60-9755-75dc9ec662c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import kenlm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "525eca5b-152d-465f-b890-aef50ebb1bd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading the LM will be faster if you build a binary file.\n",
      "Reading /Users/z5173707/root/projects/FHS/alKhalilVox/Models/ASR/LM/ngram/4gram_small.arpa\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "****************************************************************************************************\n",
      "Found entries of length > 1 in alphabet. This is unusual unless style is BPE, but the alphabet was not recognized as BPE type. Is this correct?\n"
     ]
    }
   ],
   "source": [
    "from pyctcdecode import build_ctcdecoder\n",
    "\n",
    "decoder = build_ctcdecoder(\n",
    "    labels=list(sorted_vocab_dict.keys()),\n",
    "    kenlm_model_path=lm_path_unzip,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1bd5d55a-4167-4c9b-a775-d48bd9fc5fd1",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input logits have 3 dimensions, but need 2: (time, vocabulary)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/root/projects/FHS/venv/python3.12.6/lib/python3.12/site-packages/pyctcdecode/decoder.py:716\u001b[0m, in \u001b[0;36mBeamSearchDecoderCTC.decode\u001b[0;34m(self, logits, beam_width, beam_prune_logp, token_min_logp, hotwords, hotword_weight, lm_start_state)\u001b[0m\n\u001b[1;32m    692\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecode\u001b[39m(\n\u001b[1;32m    693\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    694\u001b[0m     logits: NDArray[NpFloat],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    700\u001b[0m     lm_start_state: LMState \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    701\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[1;32m    702\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Convert input token logit matrix to decoded text.\u001b[39;00m\n\u001b[1;32m    703\u001b[0m \n\u001b[1;32m    704\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    714\u001b[0m \u001b[38;5;124;03m        The decoded text (str)\u001b[39;00m\n\u001b[1;32m    715\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 716\u001b[0m     decoded_beams \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode_beams\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    717\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    718\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeam_width\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeam_width\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    719\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeam_prune_logp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeam_prune_logp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    720\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken_min_logp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_min_logp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    721\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprune_history\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# we can set this to True since we only care about top 1 beam\u001b[39;49;00m\n\u001b[1;32m    722\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhotwords\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhotwords\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    723\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhotword_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhotword_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    724\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlm_start_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlm_start_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    725\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    726\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m decoded_beams[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/root/projects/FHS/venv/python3.12.6/lib/python3.12/site-packages/pyctcdecode/decoder.py:586\u001b[0m, in \u001b[0;36mBeamSearchDecoderCTC.decode_beams\u001b[0;34m(self, logits, beam_width, beam_prune_logp, token_min_logp, prune_history, hotwords, hotword_weight, lm_start_state)\u001b[0m\n\u001b[1;32m    560\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecode_beams\u001b[39m(\n\u001b[1;32m    561\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    562\u001b[0m     logits: NDArray[NpFloat],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    569\u001b[0m     lm_start_state: LMState \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    570\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[OutputBeam]:\n\u001b[1;32m    571\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Convert input token logit matrix to decoded beams including meta information.\u001b[39;00m\n\u001b[1;32m    572\u001b[0m \n\u001b[1;32m    573\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    584\u001b[0m \u001b[38;5;124;03m        List of beams of type OUTPUT_BEAM with various meta information\u001b[39;00m\n\u001b[1;32m    585\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 586\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_logits_dimension\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogits\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    587\u001b[0m     \u001b[38;5;66;03m# prepare hotword input\u001b[39;00m\n\u001b[1;32m    588\u001b[0m     hotword_scorer \u001b[38;5;241m=\u001b[39m HotwordScorer\u001b[38;5;241m.\u001b[39mbuild_scorer(hotwords, weight\u001b[38;5;241m=\u001b[39mhotword_weight)\n",
      "File \u001b[0;32m~/root/projects/FHS/venv/python3.12.6/lib/python3.12/site-packages/pyctcdecode/decoder.py:302\u001b[0m, in \u001b[0;36mBeamSearchDecoderCTC._check_logits_dimension\u001b[0;34m(self, logits)\u001b[0m\n\u001b[1;32m    300\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Verify correct shape and dimensions for input logits.\"\"\"\u001b[39;00m\n\u001b[1;32m    301\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(logits\u001b[38;5;241m.\u001b[39mshape) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[0;32m--> 302\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    303\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInput logits have \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m dimensions, but need 2: (time, vocabulary)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    304\u001b[0m         \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mlen\u001b[39m(logits\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m    305\u001b[0m     )\n\u001b[1;32m    306\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m logits\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_idx2vocab):\n\u001b[1;32m    307\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    308\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInput logits shape is \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m, but vocabulary is size \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    309\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNeed logits of shape: (time, vocabulary)\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (logits\u001b[38;5;241m.\u001b[39mshape, \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_idx2vocab))\n\u001b[1;32m    310\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: Input logits have 3 dimensions, but need 2: (time, vocabulary)"
     ]
    }
   ],
   "source": [
    "text = decoder.decode(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "783bd3af-d0d5-4f00-bec2-69ac69e731b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "uppercase_lm_path = '3-gram.pruned.1e-7.arpa'\n",
    "if not os.path.exists(uppercase_lm_path):\n",
    "    with gzip.open(lm_gzip_path, 'rb') as f_zipped:\n",
    "        with open(uppercase_lm_path, 'wb') as f_unzipped:\n",
    "            shutil.copyfileobj(f_zipped, f_unzipped)\n",
    "    print('Unzipped the 3-gram language model.')\n",
    "else:\n",
    "    print('Unzipped .arpa already exists.')\n",
    "\n",
    "lm_path = 'lowercase_3-gram.pruned.1e-7.arpa'\n",
    "if not os.path.exists(lm_path):\n",
    "    with open(uppercase_lm_path, 'r') as f_upper:\n",
    "        with open(lm_path, 'w') as f_lower:\n",
    "            for line in f_upper:\n",
    "                f_lower.write(line.lower())\n",
    "print('Converted language model file to lowercase.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6269073b-b23a-4cff-9522-24628777fe08",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "df9bdd96-87b4-4e60-86a0-fda1d47bea96",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Wav2Vec2ForCTC, Wav2Vec2Processor\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "ae4d1b5d-532e-44cb-8431-89f10ad67b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = 'Models/ASR/wav2vec2-large-xlsr-53-english/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "8a1ff6eb-c0dd-4a60-88c0-3e9539c2beb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/z5173707/root/projects/FHS/venv/python3.12.6/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at Models/ASR/wav2vec2-large-xlsr-53-english/ were not used when initializing Wav2Vec2ForCTC: ['wav2vec2.encoder.pos_conv_embed.conv.weight_g', 'wav2vec2.encoder.pos_conv_embed.conv.weight_v']\n",
      "- This IS expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at Models/ASR/wav2vec2-large-xlsr-53-english/ and are newly initialized: ['wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "processor = Wav2Vec2Processor.from_pretrained(model_id)\n",
    "model = Wav2Vec2ForCTC.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "658b2a67-c392-4037-a23b-ee85c38f19e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from core.utils import generate_file_basename, load_speech_file, zip_files\n",
    "from pyctcdecode import build_ctcdecoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "6248400f-839f-4688-89f8-f578e40ab3b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_dict = processor.tokenizer.get_vocab()\n",
    "sorted_vocab_dict = {k.lower(): v for k, v in sorted(vocab_dict.items(), key=lambda item: item[1])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "b89dab9a-d96d-48ee-b1e0-0dcebe2ee69e",
   "metadata": {},
   "outputs": [],
   "source": [
    "lm_path = 'Models/ASR/LM/ngram/4gram_big.arpa.gz'\n",
    "lm_path_unzip = 'Models/ASR/LM/ngram/4gram_big.arpa'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "c4948b75-f308-42ea-bb1d-3df17e57e3a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import os, shutil\n",
    "with gzip.open(lm_path, 'rb') as f_zipped:\n",
    "    with open(lm_path_unzip, 'wb') as f_unzipped:\n",
    "        shutil.copyfileobj(f_zipped, f_unzipped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "83f45624-bd37-458d-800e-ca7e8da39c49",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading the LM will be faster if you build a binary file.\n",
      "Reading /Users/z5173707/root/projects/FHS/alKhalilVox/Models/ASR/LM/ngram/4gram_big.arpa\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "****************************************************************************************************\n",
      "Found entries of length > 1 in alphabet. This is unusual unless style is BPE, but the alphabet was not recognized as BPE type. Is this correct?\n",
      "Unigrams and labels don't seem to agree.\n"
     ]
    }
   ],
   "source": [
    "from pyctcdecode import build_ctcdecoder\n",
    "#TODO: Create .bin LM\n",
    "\n",
    "decoder = build_ctcdecoder(\n",
    "    labels=list(sorted_vocab_dict.keys()),\n",
    "    kenlm_model_path=lm_path_unzip,  # either .arpa or .bin file\n",
    "     alpha=0.5,  # tuned on a val set\n",
    "    beta=1.0,  # tuned on a val set\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "e39ab167-5d46-4e0c-8320-ee79fe310426",
   "metadata": {},
   "outputs": [],
   "source": [
    "speech, sr, duration = load_speech_file('../tmp/file_6820_20240928_232234_530279.wav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "93b75227-19d1-4dd9-b1ac-891c5dd03f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = processor(speech, sampling_rate=16_000, return_tensors=\"pt\", padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "1781e695-8ea9-4a4d-8c5a-57a45851a122",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    logits = model(inputs.input_values, attention_mask=inputs.attention_mask).logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "dfe7db79-f5e7-40f1-9c5a-be2fab4e94ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_ids = torch.argmax(logits, dim=-1)\n",
    "predicted_sentences = processor.batch_decode(predicted_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "cd2340a0-d635-4479-aaff-396a5100ab92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"okay now i will pretend i'm two this is the first speakerthis is a secondy speaker first one will say hellosecond one will say hello back\"]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "7745e941-66c4-45d8-8ed0-e6874a078a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = logits.squeeze()\n",
    "x = x.cpu().detach().numpy()\n",
    "hotwords = None #[\"hello\", \"second\"]\n",
    "text = decoder.decode(x,\n",
    "                     hotwords=hotwords,\n",
    "                     hotword_weight=10.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "d30d2d99-0fb8-42af-b845-a52ca3b88cf8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"okay now i will pretend i'm two this is the first speaker this is a second speaker first one will say hellosecond one will say hello back\""
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "71169338-a564-49c5-b301-0eca272e53ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"okay now i will pretend i'm two this is the first speaker this is a second speaker first one will say hellosecond one will say hello back\""
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "035ee529-57ad-4ca7-8fcb-f4b53bb1c71d",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = logits.squeeze()\n",
    "x = x.cpu().detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5d7609f5-5fa2-4852-a960-b82f89391b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb5c3322-e9f0-4bed-9180-b13231455e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.amax(x, axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0dd4c546-dd97-4938-9be8-133c1aeff3c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 12.993846 , -17.811337 , -17.980957 , ...,  -5.951513 ,\n",
       "         -2.7360475,  -5.3542457],\n",
       "       [ 12.287314 , -16.559212 , -16.688166 , ...,  -5.4883504,\n",
       "         -2.484035 ,  -4.5250134],\n",
       "       [ 12.447207 , -16.961746 , -17.092838 , ...,  -5.403464 ,\n",
       "         -2.4596238,  -4.6880875],\n",
       "       ...,\n",
       "       [ 14.150415 , -20.84775  , -21.004705 , ...,  -6.004456 ,\n",
       "         -3.8888366,  -6.075698 ],\n",
       "       [ 13.587078 , -19.1908   , -19.345034 , ...,  -5.3483753,\n",
       "         -3.1083999,  -5.3358965],\n",
       "       [  1.1179023,  -8.682626 ,  -8.614253 , ...,  -3.0244646,\n",
       "         -1.1476758,  -4.2867713]], dtype=float32)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.cpu().detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "4b103c54-477e-4f7b-a542-13a498a4f937",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading the LM will be faster if you build a binary file.\n",
      "Reading /Users/z5173707/root/projects/FHS/alKhalilVox/Models/ASR/LM/ngram/4gram_small.arpa\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "****************************************************************************************************\n"
     ]
    }
   ],
   "source": [
    "import kenlm\n",
    "\n",
    "kenlm_model = kenlm.Model(lm_path_unzip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4c4f0d9d-f4af-481f-9b9b-bc2f42235cda",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyannote.audio as p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ea0d4135-5794-44fe-8c2e-976dcad7c74b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.3.2'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60a9175f-4d39-4753-862a-0286cc90e7b0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
